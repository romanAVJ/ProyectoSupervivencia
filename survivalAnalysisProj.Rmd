---
title: "EDA surv project"
output: html_notebook
---

```{r}
#libraries
library(dplyr)
library(ggplot2)
library(survival)
library(survminer)
library(tidyr)

#own functions for survivance analysis
source('Functions/surv_functions.R')
source('Functions/EDA_np_functions.R')

```

# EDA

Primero observemos las variables.

```{r}
# principal table
clientes <- readRDS("FinalDataSet/transacciones.rds")

# look data set
# clientes %>% head() %>% knitr::kable()
```


## Análisis Univariado

Observemos primero como se comportan las variables por si mismas y con respecto a sus distintos niveles (factores).

### Número de transacciones

Veamos un la estrucctura de las transacciones


NOTA: HACER LO MISMO PERO PARA LOS DÍAS OBSERVADOS (CON TODAS LAS OBSERVACIONES, DISTINTO AL DE ABAJO QUE ES SOLO UNA MUESTRA Y QUE SE NOTAN CENSURAS Y NO).

```{r}
# univariate analysis
#boxplot
bxplt_trans_ab <- ggplot(data = clientes) +
                geom_boxplot(mapping = aes(x = factor(Abandono),y = NumTransx, fill = factor(Abandono))) +
                  theme_minimal()

bxplt_trans_mora <- ggplot(data = clientes) +
                geom_boxplot(mapping = aes(x = factor(en_mora),y = NumTransx, fill = factor(en_mora))) +
                  theme_minimal()

#violin plot
vln_trans_ab <- ggplot(data = clientes) +
                geom_violin(mapping = aes(x = factor(Abandono),y = NumTransx, fill = factor(Abandono))) +
                  theme_minimal()

vln_trans_mora <- ggplot(data = clientes) +
                geom_violin(mapping = aes(x = factor(en_mora),y = NumTransx, fill = factor(en_mora))) +
                  theme_minimal()



#both box and violin
all_trans_mora <-  ggplot(data = clientes, mapping = aes(x = factor(en_mora),y = NumTransx)) +
                geom_boxplot() +
                geom_violin(alpha = 0.5) +
                geom_jitter(alpha = 0.04, col = 'blue') +
                  theme_minimal()

all_trans_ab <-  ggplot(data = clientes, mapping = aes(x = factor(Abandono),y = NumTransx)) +
                geom_boxplot() +
                geom_violin(alpha = 0.5) +
                geom_jitter(alpha = 0.04, col = 'blue') +
                  theme_minimal()

all_trans_edo <-  ggplot(data = clientes, mapping = aes(x = factor(Abandono),y = NumTransx)) +
                geom_boxplot() +
                geom_violin(alpha = 0.5) +
                geom_jitter(alpha = 0.04, col = 'blue') +
                facet_wrap(~ CVE_ENT) +
                  theme_minimal()

all_trans_abAndmora <- all_trans_mora +
                        facet_grid(. ~ factor(Abandono))

# density estimation
clientes$NumTransx %>% plt_dnsty(bw = 'nrd0')


# rango de pobreza
bxplt_pobreza_mora <-  clientes %>% 
                        select(NumTransx, en_mora, Abandono, NoDisponible, R_0_18, R_18_34, R_34_50, R_50_70, R_70_100, UnaVivHabitada, SinVivHabitadas) %>% 
                        gather(NoDisponible, R_0_18, R_18_34, R_34_50, R_50_70, R_70_100, UnaVivHabitada, SinVivHabitadas, key = 'Pobreza', value = 'dummie') %>% 
                        filter(dummie == '1') %>% 
                        select(NumTransx:Pobreza) %>% 
                        ggplot(mapping = aes(factor(en_mora), NumTransx)) +
                        geom_boxplot() +
                        geom_violin(alpha = 0.5) +
                        geom_jitter(alpha = 0.04, col = 'blue') +
                        facet_grid(. ~ Pobreza) +
                        theme_bw()

bxplt_pobreza_abandono <-  clientes %>% 
                        select(NumTransx, en_mora, Abandono, NoDisponible, R_0_18, R_18_34, R_34_50, R_50_70, R_70_100, UnaVivHabitada, SinVivHabitadas) %>% 
                        gather(NoDisponible, R_0_18, R_18_34, R_34_50, R_50_70, R_70_100, UnaVivHabitada, SinVivHabitadas, key = 'Pobreza', value = 'dummie') %>% 
                        filter(dummie == '1') %>% 
                        select(NumTransx:Pobreza) %>% 
                        ggplot(mapping = aes(factor(Abandono), NumTransx)) +
                        geom_boxplot() +
                        geom_violin(alpha = 0.5) +
                        geom_jitter(alpha = 0.04, col = 'blue') +
                        facet_grid(. ~ Pobreza) +
                        theme_bw()

bxplt_pobreza_all <-  clientes %>% 
                        select(NumTransx, en_mora, Abandono, NoDisponible, R_0_18, R_18_34, R_34_50, R_50_70, R_70_100, UnaVivHabitada, SinVivHabitadas) %>% 
                        gather(NoDisponible, R_0_18, R_18_34, R_34_50, R_50_70, R_70_100, UnaVivHabitada, SinVivHabitadas, key = 'Pobreza', value = 'dummie') %>% 
                        filter(dummie == '1') %>% 
                        select(NumTransx:Pobreza) %>% 
                        ggplot(mapping = aes(Pobreza, NumTransx)) +
                        geom_boxplot() +
                        geom_violin(alpha = 0.5) +
                        geom_jitter(alpha = 0.1, col = 'green') +
                        facet_grid(factor(Abandono) ~ factor(en_mora)) +
                        theme_bw()


```

### Tiempos de las observaciones

Observemos como se distribuyen los tiempos con y sin censura dependiendo de:

- Rango de pobreza.
- Mora.
- Ámbito



```{r}
#seed for random sampling
set.seed(7)
index <- sample(1:9603,size = 300,replace = F)
index2 <- sample(1:9603,size = 1000,replace = F)
#dot plot by censoring

gdot1_times <-  clientes[index,] %>%
                    mutate(DiasObsrvdos = as.integer(DiasObsrvdos)) %>% 
                    ggplot(mapping = aes(x = factor(Ambito),y = DiasObsrvdos, fill = factor(Abandono))) +
                    geom_dotplot( binaxis = 'y',stackdir = 'center', dotsize = 0.5, binwidth = 35, position = position_dodge(0.5)) +
                    geom_boxplot(position =  position_dodge(0.5), alpha = 0.2)

gdot2_times <-  clientes[index,] %>%
                    mutate(DiasObsrvdos = as.integer(DiasObsrvdos)) %>% 
                    ggplot(mapping = aes(x = factor(Ambito),y = DiasObsrvdos)) +
                    geom_dotplot( mapping = aes(fill = factor(Abandono)), binaxis = 'y',stackdir = 'center', dotsize = 0.5, binwidth = 35, position = position_dodge(0.3)) +
                    geom_boxplot(aes(fill = factor(Ambito)),alpha = 0.2)


gdot_times_pobreza <-  clientes[index2,] %>% 
                        select(DiasObsrvdos, en_mora, Abandono, NoDisponible, R_0_18, R_18_34, R_34_50, R_50_70, R_70_100, UnaVivHabitada, SinVivHabitadas) %>% 
                        gather(NoDisponible, R_0_18, R_18_34, R_34_50, R_50_70, R_70_100, UnaVivHabitada, SinVivHabitadas, key = 'Pobreza', value = 'dummie') %>% 
                        filter(dummie == '1') %>% 
                        select(DiasObsrvdos:Pobreza) %>% 
                        mutate(DiasObsrvdos = as.integer(DiasObsrvdos)) %>% 
                        ggplot(mapping = aes(Pobreza, DiasObsrvdos)) +
                        geom_dotplot( mapping = aes(fill = factor(Abandono)), binaxis = 'y',stackdir = 'center', dotsize = 0.5, binwidth = 25, position = position_dodge(0.3)) +
                        geom_boxplot(alpha = 0.2) +
                        theme_bw()

gdot1_times
gdot2_times


```


Vemos que hay una diferencia fuerte al hacer caso o no la censura en la media para ambos grupos.


Sería bueno ver proporciones de censura por grupo


### Localización

```{r}
# mexico lon, lat
clientes %>% ggplot(mapping = aes(x = lon, y = lat)) + 
        borders(xlim = c(-130, -110), ylim = c(20, 25)) +
        geom_point(col = 'red', pch = 16, alpha = 0.2)

# idea2



```


# EDA proyecto escrito


## Número de transacciones


```{r}

# individual boxplot
bxplt_tr <- clientes %>% 
            ggplot(  aes( x = '.', y = NumTransx)) +
            geom_jitter( aes( colour = factor(Abandono)), alpha = 0.1, width = 0.5 ) +
            geom_boxplot( alpha = 0.5) 

bxplt_tr2 <- clientes %>% 
            ggplot(  aes( x = '0.5', y = NumTransx)) +
            geom_jitter( aes( colour = factor(Abandono)), alpha = 0.1, width = 0.5 ) +
            geom_violin( aes( fill = factor( Abandono), group = factor( Abandono) ),alpha = 0.5, position = position_dodge(1)) +
            geom_boxplot( alpha = 0.5)

bxplt_tr_censored <- clientes %>% 
            ggplot(  aes( x = '.', y = NumTransx)) +
            geom_boxplot(aes(color = factor(Abandono)), alpha = 0.9) +
            geom_dotplot(mapping = aes(fill = factor(Abandono)),binaxis = 'y', stackdir = 'center', dotsize = 0.1, alpha = 0.1, binwidth = 5 ) 

bxplt_tr_censored2 <- clientes %>% 
            ggplot(  aes( x = factor(Abandono), y = NumTransx)) +
            geom_boxplot(aes(color = factor(Abandono)), alpha = 0.9) +
            geom_dotplot(mapping = aes(fill = factor(Abandono)),binaxis = 'y', stackdir = 'center', dotsize = 0.1, alpha = 0.1, binwidth = 5 ) 



# kernel denisty and histogram
dsty_tr <- clientes %>% 
            ggplot( aes(x = NumTransx) ) +
            geom_histogram(aes(y = ..density..), fill = '#BEDDE4', color = 1  ) +
            geom_density( kernel = 'epanechnikov', color = 'blue') 

bxplt_tr
bxplt_tr2
bxplt_tr_censored
bxplt_tr_censored2
dsty_tr



```

## Días observados


```{r}
# box plot


bxplt_days1 <- clientes %>% 
              ggplot(aes(x = '.', y = DiasObsrvdos)) +
              geom_jitter( aes( color = factor( Abandono)), alpha = 0.1, width = 0.5) +
              geom_violin( alpha = 0.5) + 
              geom_boxplot( alpha = 0.5 ) 




bxplt_days2 <- clientes %>% 
              ggplot(aes(x = factor(Abandono), y = DiasObsrvdos)) +
              geom_boxplot(aes( color = factor(Abandono)), ) +
              geom_dotplot( mapping = aes( fill = factor( Abandono)) , binaxis = 'y', stackdir = 'center', dotsize = 0.5, alpha = 0.1, binwidth = 5)


# denisty
dsty_days <- clientes %>% 
            ggplot( aes(x = DiasObsrvdos) ) +
            geom_histogram(aes(y = ..density..), fill = '#BEDDE4', color = 1  ) +
            geom_density( kernel = 'epanechnikov', color = 'blue', size = 1, bw = 'SJ-dpi') +
            geom_density( kernel = 'gaussian', color = 'green', size = 1, bw = 30) 

dsty_days_c1 <- clientes %>% 
                ggplot( aes(x = DiasObsrvdos) ) +
                geom_density( mapping =  aes( fill = factor( Abandono)), kernel = 'epanechnikov', position = 'stack')

dsty_days_c2 <- clientes %>% 
                ggplot( aes(x = DiasObsrvdos) ) +
                geom_density( mapping =  aes( fill = factor( Abandono)), kernel = 'epanechnikov', alpha = 0.4)


bxplt_days1
bxplt_days2
dsty_days
dsty_days_c1
dsty_days_c2





```

# Análisis de supervivencia

A continuación estimemos la supervivencia de los usuarios.

## Sin covariables

Estimacion

```{r}

# total data
xfit <- survfit( Surv(time = DiasObsrvdos, event = Abandono) ~ 1, 
                 data = clientes, conf.type = 'plain', # linear conf int
                 start.time = 0)




# trans
xfit_t <- survfit( Surv(time = DiasObsrvdos, event = Abandono) ~ NumTransx, 
                 data = clientes, conf.type = 'plain', # linear conf int
                 start.time = 0)

```

Notas: La estimacion KM con covariables numéricas, cambia muchísimo por la gran cantidad de "niveles". Hay dos opciones:

- Recodificar y dejar pocas categorias
- 



Tablas de supervivencia
```{r}
table_km <- clientes %>% mutate( time = DiasObsrvdos, censored = Abandono) %>% table_KM_surv()

# observe tables
# table_km %>% head() %>% knitr::kable()
# table_km %>% tail() %>% knitr::kable()

```

## KM plots

```{r}
#plot 1
gg_surv_km <- ggsurvplot(xfit, 
            surv.median.line = c('hv'),
            conf.int = TRUE,
            linetype = 'solid',
            censor.shape = 39,
            legend.title = 'Tiempo de Churn',
            legend.labs = '',
            xlab = 'Dias',
            ylab = 'Probabilidad de abandono')

# plot 2
plot(xfit, col = c(1,2,2)) #i think this is better

 # ggsurvplot(xfit_t, 
 #            surv.median.line = c('hv'),
 #            legend.title = 'Tiempo de Churn',
 #            xlab = 'Dias',
 #            ylab = 'Probabilidad de abandono')

gg_surv_km
```

## Gráficas diagnositico para modelos de regresión

Al ser una variable numérica, debemos ver los tiempos sin la covariable

En el caso que se reetiquete el número de transacciones, se puede hacer un análisis parecido al del profesor para el caso del voltaje.

### Sin covariables
```{r}
# time and KM
tj <- xfit$time
Sj <- xfit$surv
```


#### QQ plots

__Log normal__

```{r}
ln_T <- scale(log(tj))

#graph1
qqnorm(ln_T)

#graog2
g_qqnorm <- tibble( tj = ln_T) %>% 
            ggplot(aes(sample = tj)) +
            stat_qq(shape = 1, color = '#808080') + 
            stat_qq_line( color = '#008080', size = 1)
g_qqnorm

```

__Weibull__

#### Linearización del modelo

 There are multiple ways to parameterize a Weibull distribution. The survreg 
 function embeds it in a general location-scale family, which is a 
 different parameterization than the rweibull function, and often leads

Importante:
  survreg's scale  =    1/(rweibull shape)
   survreg's intercept = log(rweibull scale)

En r, _shape_ es $a$ y _scale_ es $b$ de la siguiente parametrización:

$$ f(x) = (a/b) (x/b)^{a-1} exp(- (x/b)^a)$$

-- no sirve de mucho --

```{r}
xfitr_weib <- survreg(Surv(time = DiasObsrvdos, event = Abandono) ~ 1,  data = clientes, dist = 'weibull')

#adjusted values
a <- 1 / xfitr_weib$scale
b <- exp(xfitr_weib$coef) 

#graph
x <- seq(from = 0, to = 10, length.out = 2000)
y <- dweibull(x = x, shape = a, scale = b)

plot(x,y) #look weibull dist

#qqweibull
n <- length(Sj)
pp <- seq(from = 0, to = 1, length.out = n)
tjp <- qweibull(p = pp, shape = a, scale = b)


```




__ Exponencial __

```{r}

# grph 1
g_dgs_exp <- tibble(tj, Sj) %>% 
              ggplot( ) +
              geom_line( mapping = aes( x = tj, y = log(Sj) ))


# adjust line (maybe with a grid)
# g_dgs_expL <- g_dgs_exp +
#               geom_line( mapping = aes( x = ) )
# 
g_dgs_exp



```

__ Weibull __

```{r}
g_dgs_wbl <- tibble( tj, Sj) %>% 
              ggplot() +
              geom_line( mapping = aes( log(tj), log( - log( Sj))) )

g_dgs_wbl


```

Por las gráficas anteriores, los datos sugieren un modelo Exponencial.


## Ajuste de los modelos

```{r}
#fits of distributions saw in class
xfitr_weib <- survreg(Surv(time = DiasObsrvdos, event = Abandono) ~ 1,  data = clientes, dist = 'weibull')
xfitr_ll<- survreg(Surv(time = DiasObsrvdos, event = Abandono) ~ 1,  data = clientes, dist = 'loglogistic')
xfitr_ln <- survreg(Surv(time = DiasObsrvdos, event = Abandono) ~ 1,  data = clientes, dist = 'lognormal')
xfitr_e <- survreg(Surv(time = DiasObsrvdos, event = Abandono) ~ 1,  data = clientes, dist = 'exponential')
xfitr_ex <- survreg(Surv(time = DiasObsrvdos, event = Abandono) ~ 1,  data = clientes, dist = 'extreme')


xfitr_weib
xfitr_ll
xfitr_ln
xfitr_e
xfitr_ex

```


__NOTA__ A pesar que los datos visualmente se ajustaban mejor para la exponencial, la lognormal (la mejor por poco) y la weibull son las mejores aproximaciones paramétricas a los datos ajustados. Veremos de todos modos el criterio de Akaike para verlo.

```{r}
extractAIC(xfitr_weib)
extractAIC(xfitr_ln)
extractAIC(xfitr_e)
extractAIC(xfitr_ex)
extractAIC(xfitr_ll)


```

Por lo visto, se reafirma que el mejor modelo es el lognormal seguido del weibull por poco.

Última comprobación es con ver cómo se ajustan las curvas con el el estimado del KM

```{r}
# quantiles
n <- xfit$n

# predict
qs <- 1:98/100
predw <- predict(xfitr_weib, type="quantile", p = 1- qs, se = TRUE)
predln <- predict(xfitr_ln, type="quantile", p = 1- qs, se = TRUE)
prede <- predict(xfitr_e, type="quantile", p = 1- qs, se = TRUE)
predll <- predict(xfitr_ll, type="quantile", p = 1- qs, se = TRUE)
predex <- predict(xfitr_ex, type="quantile", p = 1- qs, se = TRUE)


plot( xfit)
lines( predw$fit[1,], qs, col = 2 )
lines( predln$fit[1,], qs, col = 3 )
lines( prede$fit[1,], qs, col = 4 )
lines( predll$fit[1,], qs, col = 5 )
lines( predex$fit[1,], qs, col = 6 )


```

Sin embargo, el mejor ajuste en la curva es el modelo exponencial. (Se puede ajustar la prueba Kolomgorov Smirnoff para aumentar la confianza estadística).

Prueba kolmogorv
```{r}
# got stuck

# ks.test(tj, 1-'pweibull', ,alternative = 'two.side')
```


## Covariables

### Num Trans Numérica y Ámbito

En esta parte veremos como se ajustan los modelos weibull y lognormal con las covariables _Numero de trans_ y _Marginación_. (Urbana, Rural)

```{r}
sfitr_w <- survreg(Surv(time = DiasObsrvdos, event = Abandono) ~ NumTransx + factor(Ambito), data = clientes, dist = 'weibull')

# (ponerlo en bonito)
summary(sfitr_w)
anova(sfitr_w)
```


Por la tabla de ANOVA, vemos que ambos factores aportan información de manera marginal.

-- Falta interpretar los coeficientes de la regresión --

Observemos las curvas para distintas predicciones

```{r}
# setp 1: parametric model adjust 

# setp 2: variables to predict
newdat <- expand.grid( NumTransx = c(1, 5, 10), 
                      Ambito = levels(as.factor(clientes$Ambito)))

# step 3: compute several curves
surv <- seq(.99, .1, by = -.01)
ti <- predict(sfitr_w, type = 'quantile', p = 1 - surv, newdata = newdat)

# setp4: create df with sruvival curves
newdat_wide <- cbind(newdat, ti)
surv_df <- reshape2::melt(newdat_wide,
                          id.vars = c('NumTransx','Ambito'),
                          variable.name = 'surv_id',
                          value.name = 'time')
surv_df$surv <- surv[ as.numeric(surv_df$surv_id)]
surv_df[,c('upper', 'lower', 'std.err', 'strata')] <- NA

# step5: plot

# survival
g1 <- ggsurvplot_df( surv_df,
               surv.geom = geom_line,
               linetype = 'Ambito',
               color = 'NumTransx',
               legend.title = NULL,
               surv.median.line = c('hv'))

# distribution

g2 <- ggsurvplot_df( surv_df,
               fun = 'event',
               surv.geom = geom_line,
               linetype = 'Ambito',
               color = 'NumTransx',
               legend.title = NULL,
               surv.median.line = c('hv'))

# cummulative hazard

g3 <- ggsurvplot_df( surv_df,
               fun = 'cumhaz',
               surv.geom = geom_line,
               linetype = 'Ambito',
               color = 'NumTransx',
               legend.title = NULL,
               surv.median.line = c('hv'))

g1
g3

```


### Residuos Cox Snell modelo parametrico

Basados en los apuntes (poner fórmula) y http://blogs2.datall-analyse.nl/2016/02/19/rcode_martingale_residuals_aft_model/

```{r}
# time
tj <- clientes$DiasObsrvdos %>% as.numeric()

#null regression model
nmod <- survreg( Surv(time = DiasObsrvdos, event = Abandono) ~ 1, data = clientes, dist = 'weibull')

# standarized residuals
linfit <- predict( nmod, type = 'lp') #linear predictor
zi <-  (log( tj) - linfit) / nmod$scale


# cox snell (from functions)
coxsnellres_w <- CoxSnellResidual( zi)

# using Nelson Aalen to estimate cummulatieve hzazard
fitresw <- survfit( coxph( Surv( coxsnellres_w, clientes$Abandono) ~ 1), type = 'fleming-harrington') # NA S(t)
Ht <- - log( fitresw$surv )


# plot 
plot( fitresw$time, Ht, type = 's', xlab = 'Cox-Snell Residuales', ylab = 'H(t)')
abline(a = 0, b = 1, col = 'navy' )

```

Podemos observar un buen ajuste en el modelo paramétrico.


# Modelo de Cox

Ajustaremos un modelo semiparamétrico para los datos con las mismas covariables y veremos su desempeño.

Según la documentación, usamos el _método de Breslow_ (como lo visto en clase).

```{r}
# estimate cox model
clientes <- clientes %>% mutate(Ambito = factor(Ambito))
cfit <- coxph( Surv( time = DiasObsrvdos, event = Abandono) ~ NumTransx + Ambito, data = clientes)

# look model fit
summary(cfit)


```

Vemos que se rechaza las hipótesis que los parámetros sean cero de manera significativa y que el modelo en general sea constante. Por tanto tenemos un excelente ajuste de modelo.

Igualemente ningún parámetro tiene su correspondiente IC pasando por el 1, por tanto son distintos de cero.

A continuación vemos como se vé el modelo de Cox para estos datos

```{r}
# step 1: cox fit

# step 2: combination of covariables
newdat <- expand.grid( NumTransx = c(1, 5, 10), 
                      Ambito = levels(as.factor(clientes$Ambito)))

rownames( newdat) <- letters[1:6] #6 combinations 

# step 3: compute curves
cxsf <- survfit( cfit, data = clientes, newdata = newdat, conf.type = 'none')

# step 4: create data frame
surv_cx <- surv_summary(cxsf)


surv_csmod <- cbind(surv_cx, newdat[ as.character( surv_cx$strata), ])

# step 5: plot
ggsurvplot_df( surv_csmod,
              linetype = 'Ambito',
              color = 'NumTransx',
              legend.title = NULL,
              censor = FALSE)


```

Overal plot
```{r}

ggsurvplot( survfit(cfit), data = clientes, censor = FALSE)


```



## Analisis de residuales del modelo Cox







### Parametros
 
```{r}
ggforest(cfit, data = clientes)

```



### Residuales

#### Test para riesgos proporcionales
Residuales schoenfeld



```{r}
# schoenfeld residuals
test.res <- cox.zph( cfit)

# plot schoenfeld residuals
test.res
ggcoxzph( test.res)

```

Por la prueba de Schoenfeld, vemos que el supuesto de riesgos proporcionales NO es válido globalmente, solo para el ámbito urbano puede ser factible.

según http://www.sthda.com/english/wiki/cox-model-assumptions :

Note that, systematic departures from a horizontal line are indicative of non-proportional hazards, since proportional hazards assumes that estimates β1,β2,β3 do not vary much over time.

En estas gráficas SI vemos que las covariables cambian con el tiempo en el caso del número de transacciones, lo cual es de esperarse.

Igualmente, la interpretaión de no centralidad al rededor del 0 http://www.math.ucsd.edu/~rxu/math284/slect9.pdf afirma de nuevo esto.


```{r}

ggcoxdiagnostics( cfit, type = 'martingale')
ggcoxdiagnostics( cfit, type = 'deviance', linear.predictions = FALSE, alpha = 0.4)


```
Aunque la devianza pareciera estar cercana al cero.


#### Residuos cox snell

Según https://rpubs.com/kaz_yos/resid_cox y http://biostat.mc.vanderbilt.edu/wiki/pub/Main/QingxiaChen/Ch11.pdf (el de la foto de clase), para obtener lo residuos cox-snell.

Cómo se calucla un resiudo martingala: https://www.jstor.org/stable/2336057?seq=2#metadata_info_tab_contents 

Calculamos el residuo Cox Snell a partir de lo siguiente:

1. Calculamos los residuos martingala del modelo de cox.
2. Equivalencia $r_{cs}^{i} = \delta^{i} - r_{M}^{i} $
3. Calculamos $e_{i} = H(r_{cs}^{i} | x_{i}, \theta_{i}) $ vía Fleming Harrington $H(t) = - ln( S(t) )$


```{r}
# internet method
coxsnellres <- clientes$Abandono - resid( cfit, type = "martingale" )


# using Nelson Aalen to estimate cummulatieve hzazard
fitres <- survfit( coxph( Surv( coxsnellres, clientes$Abandono) ~ 1), type = 'fleming-harrington') # NA S(t)
Ht <- - log( fitres$surv )


# plot 
plot( fitres$time, Ht, type = 's', xlab = 'Cox-Snell Residuales', ylab = 'H(t)')
abline(a = 0, b = 1, col = 'navy' )



```

Se puede ver el pobre ajuste para el modelo semiparametrico.

## Modelo Cox con covariables dependientes del tiempo: Modelo Aditivo de Aalen

Ajuste

```{r}
# fit model
afit <- aareg( Surv(time = DiasObsrvdos, event = Abandono) ~ NumTransx + Ambito, data = clientes)

# summary
summary(afit)

```

Graficar

```{r}
plot(afit)

```

Con el modelo aditivo de Aalen podemos ver como el Número de transacciones es dependiente del tiempo, como la misma intuición lo dice, mientras que l ámbito no. Es por esto que un modelo semiparamétrico no es el correcto para este tipo de datos.

# Comparar los tres modelos

A continuación graficaremos los ajustes KM, regresión paramétrica y el modelo de Cox.
http://rstudio-pubs-static.s3.amazonaws.com/5560_c24449c468224fd4af9f3e512a24e07d.html

## Ajustes Nulos

El único que se queda con el peso de las covariables es el modelo de Cox

```{r}
# survival curves withour covariates
km.null <- survfit( Surv( time = DiasObsrvdos, event = Abandono) ~ 1, data = clientes, conf.type = 'plain')
reg.null <- survreg( Surv( time = DiasObsrvdos, event = Abandono) ~ 1, data = clientes, dist = 'weibull')

# predictions
qs <- 1:98/100
predr <- predict(reg.null, type = 'quantile', p = 1- qs, se = TRUE)


# plot
plot( km.null, conf.int = 'none') #KM
lines( predr$fit[1,], qs, col = 2) # weibull
lines( survfit( cfit), conf.int = 'none', col = 3)
## Add legends
legend(x = "topright",
       legend = c("Kaplan-Meier", "Cox (Efron)", "Weibull"),
       lwd = 2, bty = "n",
       col = c("black", "green", "red"))


```


















